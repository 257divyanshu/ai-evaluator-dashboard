# ğŸ§  AI Evaluator Dashboard  
A lightweight research dashboard to evaluate, compare, and analyze large language model (LLM) outputs â€” scientifically and visually.  

---

# ğŸ“– Overview  
**AI Evaluator Dashboard** is a modern web application designed to assess the performance of large language models (LLMs) using both **manual** and **automated** evaluation techniques.  

It provides an intuitive interface to rate AI responses on key metrics â€” **accuracy, clarity, relevance, helpfulness** â€” and visualize results with rich analytics. Built as part of a hands-on exploration into AI evaluation workflows, this project bridges research and usability for developers, students, and AI practitioners.

---

ğŸ‘‰ **Try it here:** [https://ai-evaluator-dashboard.vercel.app](https://ai-evaluator-dashboard.vercel.app)

---

# ğŸ§° Tech Stack  
| Category | Technology |
|-----------|-------------|
| **Frontend Framework** | React (Vite) |
| **Styling** | Tailwind CSS |
| **Charts & Visuals** | Recharts |
| **LLM API** | Google Gemini (Generative AI SDK) |
| **State Persistence** | LocalStorage (via custom hook) |
| **Hosting** | Vercel |

---

# ğŸ¯ Core Features  

- ğŸ§© **Manual Evaluation** â€” Score AI responses manually across key metrics  
- ğŸ¤– **LLM-as-a-Judge (Auto Evaluation)** â€” Uses Gemini to evaluate AI outputs automatically  
- ğŸ“Š **Analytics Dashboard** â€” Visualize performance trends, model comparisons & safety distribution  
- ğŸ’¾ **Local Persistence** â€” Evaluations auto-saved in browser storage  
- ğŸ“¤ **Export Options** â€” Export all evaluations as CSV or JSON  
- âš™ï¸ **Model Selector** â€” Choose from multiple Gemini models (Flash, Pro, Lite)  
- ğŸ’¬ **Notes + Tooltips** â€” Add evaluator comments and view detailed context without clutter  
- ğŸ§  **Responsive & Clean UI** â€” Designed for research productivity across devices  

---

# ğŸ”¬ Advanced Evaluation Techniques  

This dashboard implements and explores several **industry-grade evaluation methods** for LLMs.  
Currently, **LLM-as-a-Judge** is fully implemented, and other advanced techniques are in active development.  

| Technique | Description | Status | Blog |
|------------|--------------|--------|------|
| **LLM-as-a-Judge** | Use one LLM to evaluate another modelâ€™s outputs objectively. | âœ… Implemented | [Read Blog â†’](https:///divyanshusahu.com/blog/llm-as-judge)
| **Embedding Similarity** | Measure semantic similarity between model responses and ideal references using vector embeddings. | ğŸ§© Coming Soon | [Read Blog â†’](https://divyanshusahu.com/blog/embedding-similarity) |
| **Self-Consistency Check** | Assess output reliability by comparing multiple generations for the same prompt. | ğŸ§© Coming Soon | [Read Blog â†’](https://divyanshusahu.com/blog/self-consistency-check) |
| **Automated Bias & Safety Detection** | Detect bias, toxicity, or unsafe model responses automatically. | ğŸ§© Coming Soon | [Read Blog â†’](https://divyanshusahu.com/blog/automated-bias-and-safety-detection) |
| **Prompt Tagging System** | Categorize prompts (reasoning, factual, creative, etc.) to analyze behavior by type. | ğŸ§© Coming Soon | [Read Blog â†’](https://divyanshusahu.com/blog/prompt-tagging-system) |

---

# âš™ï¸ Installation & Setup  

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/ai-evaluator-dashboard.git
cd ai-evaluator-dashboard
````

### 2ï¸âƒ£ Install Dependencies

```bash
npm install
```

### 3ï¸âƒ£ Add Environment Variables

Create a `.env` file in the project root:

```
VITE_GEMINI_API_KEY=your_gemini_api_key_here
```

*(Ensure `.env` is in your `.gitignore`!)*

### 4ï¸âƒ£ Run Locally

```bash
npm run dev
```

Visit [http://localhost:5173](http://localhost:5173)

---

# ğŸ“¦ Build & Deployment

### Build for Production

```bash
npm run build
```

### Preview Build

```bash
npm run preview
```

### Deployment

This project is deployed on **Vercel** using continuous integration from GitHub.
Set your environment variable (`VITE_GEMINI_API_KEY`) in the Vercel dashboard under *Project Settings â†’ Environment Variables*.

---

# ğŸ§‘â€ğŸ’» Project Structure

```
src/
â”£ assets/
â”ƒ â”— react.svg
â”£ components/
â”ƒ â”£ AutoEvaluator.jsx
â”ƒ â”£ EvaluationPanel.jsx
â”ƒ â”£ EvaluationTable.jsx
â”ƒ â”£ Footer.jsx
â”ƒ â”£ ModelSelector.jsx
â”ƒ â”£ Navbar.jsx
â”ƒ â”£ PromptInput.jsx
â”ƒ â”£ ResponseViewer.jsx
â”ƒ â”— Toaster.jsx
â”£ hooks/
â”ƒ â”— useLocalStorage.jsx
â”£ pages/
â”ƒ â”£ Analytics.jsx
â”ƒ â”£ Evaluate.jsx
â”ƒ â”— Home.jsx
â”£ App.css
â”£ App.jsx
â”£ index.css
â”— main.jsx
```

---

# ğŸ“ˆ Future Enhancements

* âœ³ï¸ Add **Embedding Similarity** metric
* â™»ï¸ Implement **Self-Consistency Check** for reliability evaluation
* ğŸ” Integrate **Bias & Safety Detection** with visual insights
* ğŸ·ï¸ Add **Prompt Tagging System** for categorical analytics
* â˜ï¸ Optional **cloud sync** for evaluation history
* ğŸ”„ **Dual-model comparison mode** for side-by-side LLM testing

---

# ğŸ¤ Contributing

Contributions and ideas are welcome!
If you'd like to collaborate, open an issue or a pull request describing your proposed improvements.

---

ğŸ”— [LinkedIn Profile](https://www.linkedin.com/in/257divyanshu/)
ğŸ”— [My Blog](https://divyanshusahu.com/blog)
